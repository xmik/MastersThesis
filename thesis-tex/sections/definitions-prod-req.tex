\subsection{Production deployment requirements}
\textit{This section explains what a production deployment is and what requirements must it met.}
~\\
~\\

\subsubsection{Multiple environments in software deployment}
First, it is helpful to distinguish between the terms: \textbf{'infrastructure stack' and 'environment'}. They both may be defined as a collection of infrastructure resources. The difference is that an environment is conceptual, while a stack is concrete. A stack is defined with code, particularly when Infrastructure as Code is applied, and managed using tools. However, an environment serves to fulfill a predetermined purpose. Multiple environments can run an instance of the same system\footnote{\cite{book-iac}, p. 189}.

Typically, there are \textbf{two reasons for which multiple environments are in use}: to support a release delivery process and to run multiple production instances of the system. The first reason allows to have a particular build of an application (e.g. a git commit or a specified version of code) well tested. Such a build has to go through many different environments, e.g.: testing, staging and production. When a build does not pass all the stages in the former environments, it will not be promoted to the production environment\footnote{\cite{book-iac}, p. 190, \cite{book-cicd}, p. 254}. An example release process diagram is depicted below:
\begin{figure}[H]
    \centering
    \includegraphics[width=16cm]{figures/cicd-example-release-diagram.png}
    \label{fig:cicd-example-release-diagram}
    \caption{An example test and release process diagram}
    \small{Source: \cite{book-cicd}, p. 255}
\end{figure}

To briefly explain the second reason for multiple environments: they are used in order to ensure fault-tolerance (when one environment fails, the other can take over), scalability (the work can be spread among many clusters), segregation (it may be decided to handle a group of customers using one environment and the other group with the other environment, e.g. for latency purposes)\footnote{\cite{book-iac}, p. 192}.

\subsubsection{Production deployment requirements}
Throughout this work a production deployment means such a deployment which targets the production environment. A list of \textbf{requirements for a production deployment}, gathered through the literature, is provided below:
\begin{itemize}
\item \textbf{Central Monitoring} - this is helpful when troubleshooting a cluster\footnote{\cite{book-mastering-k8s}, p. 41-47, 60, \cite{online-weave-checklists}, p. 5, \cite{online-weave-guide}, p. 2, \cite{book-cndwk}, p. 38}.
\item \textbf{Central Logging} - this is a fundamental requirement for any cluster with number of nodes or pods or containers greater than a couple\footnote{\cite{book-mastering-k8s}, p. 55-58, \cite{online-weave-checklists}, p. 6, \cite{book-devops-k8s}, p. 851}.
\item \textbf{Audit} - to show who was responsible for what action\footnote{\cite{online-weave-guide}, p. 6}.
\item \textbf{High Availability} - authors of \cite{book-mastering-k8s} go even further and state that the cluster should be tested for being reliable and highly available \textbf{before} it is deployed into production\footnote{\cite{book-mastering-k8s}, p. 65-80, \cite{book-cndwk}, p. 37}.
\item \textbf{Live cluster upgrades} - it is not affordable for large Kubernetes clusters with many users to be offline for maintenance\footnote{\cite{book-mastering-k8s}, p. 80}.
\item \textbf{Backup, Quick recovery or even Zero down-time}\footnote{\cite{book-mastering-k8s}, p. 87, \cite{online-weave-guide}, p. 2, \cite{book-cndwk}, p. 38}.
\item \textbf{Security, secrets management, image scanning} - security at many levels is needed (node, image, pod and container, etc.)\footnote{\cite{book-mastering-k8s}, p. 91-97, \cite{online-weave-checklists}, p. 5, 6, \cite{online-weave-guide}, p. 2, 4-6, \cite{book-cndwk}, p. 38}.
\item \textbf{Passing tests, a healthy cluster} - "if you don't test it, assume it doesn't work"\footnote{\cite{book-mastering-k8s}, p. 78, \cite{book-cndwk}, p. 39}.
\item \textbf{Automation and Infrastructure as Code} - in production environment a versioned, auditable, and repeatable way to manage the infrastructure is needed\footnote{\cite{book-mastering-k8s}, p. 269, \cite{online-weave-guide}, p. 2}.
\end{itemize}

\subsubsection{Requirements explanations and how to satisfy them}
\textbf{Monitoring} helps to ensure that a cluster is operational, correctly configured and that there are enough resources deployed. Monitoring is also indispensable for debugging and troubleshooting\footnote{\cite{book-mastering-k8s}, p. 41}. The third reason for using a monitoring system is that historical data is needed for planning purposes. The monitoring strategy should cover four areas\footnote{\cite{book-cicd}, p. 317}:
\begin{itemize}
\item configuring the infrastructure in such a way that it is possible to collect the data
\item storing the data
\item providing dashboards, so that data is presented in a clear way
\item setting up notifications or alarms to let people know about certain events
\end{itemize}
\paragraph{}
Monitoring manages the following data: CPU usage, memory utilization, I/O per disk, disk space, number of network connections, response time, etc. Thus, it is helpful on many different levels: on hardware, operating system,  middleware and application level.
There is a wide range of available open source and commercial tools: Nagios, OpenNMS, Flapjack, Zenoss, Tivoli from IBM, Operations Manager from HP, Splunk, etc.\footnote{\cite{book-cicd}, p. 318}. Solutions recommended for a Kubernetes cluster is: Heapster combined with InfluxDB as backend and Grafana as frontend and there is also cAdvisor\footnote{\cite{book-mastering-k8s}, p. 42, 43}. A nice feature of Grafana are its dashboards. Example Grafana dashboard is presented in the next image:
\begin{figure}[H]
  \centering
  \includegraphics[width=11cm]{figures/grafana.png}
  \label{fig:grafana}
  \caption{Grafana dashboard for Kubernetes cluster}
  \\
  \small{Source: \url{https://linoxide.com/linux-how-to/monitor-kubernetes-cluster-prometheus-grafana/}, (accessed: 15.04.2020)}
\end{figure}
Grafana also works well with Prometheus, which is a monitoring system and a time series database. Prometheus is also a CNCF graduated project\footnote{\cite{online-prometheus-gh}, \cite{online-prometheus-www}}. When a deployment happens on AWS, another solution for monitoring and logging may be: Amazon CloudWatch\footnote{\cite{online-cw}}. There is also Kubernetes dashboard, which is a built-in solution and doesn't require any customization. Heapster, InfluxDB and Grafana are great for heavy-duty purposes, whereas Kubernetes dashboard is probably able to satisfy the majority of monitoring needs of a Kubernetes cluster\footnote{\cite{book-mastering-k8s}, p. 48, \footnote{\cite{book-devops-k8s}, p. 875, 879-882}}. Example dashboard provided by Kubernetes dashboard is depicted on the next image:
\begin{figure}[H]
  \centering
  \includegraphics[width=10cm]{figures/k8s-dashboard.png}
  \label{fig:grafana}
  \caption{Kubernetes dashboard in action}
  \\
  \tiny{Source: \cite{book-mastering-k8s}, p. 53}
\end{figure}

Another advantage of Kubernetes dashboard is that it can show \textbf{log messages} of a single container deployed on Kubernetes\footnote{\cite{book-mastering-k8s}, p. 54}. \textbf{Centralized logging} is essential for a production cluster, because usually there are a lot of pods (and containers) deployed, each generating many log messages. It is impossible to require a Kubernetes administrator to log into each container for the purpose of getting the logs. The second reason for the importance of centralized logging is that containers are ephemeral - the log messages kept inside the containers would be lost after a container is redeployed. A popular solution are: Fluentd, Elasticsearch, Kibana\footnote{\cite{book-mastering-k8s}, p. 55-57}, Logstash\footnote{\cite{book-devops-k8s}, p. 851, 854} and Graylog\footnote{\cite{online-prod-year-k8s}, \cite{online-graylog}}. It is also important to consider that log messages in Kubernetes cluster are generated from many sources: from end-user applications, nodes, Kubernetes system containers and there are also \textbf{audit logs} in the form of e.g. api server events\footnote{\cite{online-graylog-art}}. For the purposes of auditing, when deploying on AWS, one can use AWS CloudTrail\footnote{\cite{online-ct}}.

While administering a Kubernetes cluster, there is a high probability that something will go wrong. Components, network can fail, configuration can be incorrect, people make mistakes and software has bugs. Failure classification has been described in \cite{article-failures}, p. 5. This has to be accepted and a system should be designed in such a way that it is \textbf{reliable and highly available (HA)} despite of many problems. Here is a list of ideas how to ensure high availability\footnote{\cite{book-mastering-k8s}, p. 66}:
\begin{itemize}
\item \textbf{Redundancy} - means having a spare copy of something. Kubernetes uses Replica Sets or Replication Controllers to provide redundancy for applications deployed on Kubernetes. Five redundancy models were summarized in \cite{article-redundancy-models}, p. 978-980. Some of them require an active replica (running) and other passive (or standby).
\item \textbf{Hot Swapping} - can be explained as replacing some failed component on the fly, with minimal or ideally zero down-time. Actually, hot swapping is quite easy to implement for stateless applications. For stateful applications, one has to keep a replica of a component (see redundancy).
\item \textbf{Leader election} - it is a pattern used in distributed systems. Whenever there are many servers fulfilling the same purpose to share the load. One of the servers must be elected a leader then and certain operations must go through it. When the leader server experiences a failure, other server can be selected as new leader. This is a combination of redundancy and hot swapping.
\item \textbf{Smart load balancing} - used to share and distribute the load.
\item \textbf{Idempotency} - means that one request (or some operation) is handled exactly once.
\item \textbf{Self-healing} - means that whenever a failure of one component happens, it is automatically detected and steps are taken (also automatically) to get rid of the failure.
\item \textbf{Deploying in a cloud} - a goal is to be able to physically remove or replace a piece of hardware, either because of some issues or because of preventative maintenance or horizontal growth. Often this is too expensive or even impossible to achieve\footnote{\cite{article-failures}, p. 14}. Traditional deployments on-premises forced administrators to do a capacity planning (to predict the amount of computing resources). Thanks to the on-demand and elastic nature of the clouds, the infrastructure can be closely aligned to the actual demand. It is also easy to scale applications deployed on a cloud, because of the fundamental property of the cloud: elasticity\footnote{\cite{article-aws-architecting}, p. 9}.
\end{itemize}

\paragraph{}
Generally speaking, \textbf{"highly available systems are fault tolerant systems with no single point of failure"\footnote{\cite{article-redundancy-models}, p. 975}}. In order to introduce HA for the Kubernetes cluster the following ideas could be incorporated\footnote{\cite{book-mastering-k8s}, p. 68}:
\begin{itemize}
\item Deploy Etcd as a cluster, not just one instance of Etcd
\item Ensure redundancy for api server
\item Deploy multiple master instances and ensure a load balancer in front of them
\item Ensure that node instances are reliable: the Docker daemon and the Kubelet daemon should restart automatically in case of failure
\item Apply RAID to ensure redundancy of data storage or apply Key-Value Multi-Device (KVMD), a hybrid data reliability manager\cite{data-rel-kv} or let cloud provide storage availability
end{itemize}

\paragraph{}
Furthermore, it may be needed to test high availability. This can be done by inducing a predictable failure and verifying if the system behaves as expected\footnote{\cite{book-mastering-k8s}, p. 79}.

\paragraph{}
When it comes to \textbf{automation}, many guidelines can be found in \cite{book-cicd}, p. 13-29. Below are some of them listed:
\begin{itemize}
\item \textbf{Every Change Should Trigger the Feedback Process} - means that every change in code should trigger some pipeline and should be tested (including unit tests, functional acceptance tests, nonfunctional tests). The tests should happen in an environment which is as similar as possible to production. Some tests may run in production environment too\footnote{\cite{book-cicd}, p. 13,14, \cite{book-iac}, p. 284, 285}.
\item \textbf{The Feedback Must Be Received as Soon as Possible} - this also involves another rule: fail fast. This guideline suggests that faster tests (or less resource-intensive tests) should run first. If theses tests fail, the code does not get promoted to the next pipeline stages, which ensures optimal use of resources\footnote{\cite{book-cicd}, p. 15}.
\item \textbf{Automate Almost Everything} - generally, the build process should be automated to such extent where specific human intervention or decision is needed. But there is no need to automate everything at once\footnote{\cite{book-cicd}, p. 25, \cite{book-iac}, p. 284}.
\item \textbf{Keep Everything in Version Control} - this means that not only application source code but also tests, documentation, database configuration, deployment scripts, etc. should be kept in version control and that it should be possible to identify the relevant version. Furthermore, any person with access to the source code should be able to invoke a single command in order to build and deploy the application to any accessible environment. Apart from that, it should be also clear which version in version control was deployed into each environment\footnote{\cite{book-cicd}, p. 26}.
\item \textbf{If It Hurts, Do It More Frequently, and Bring the Pain Forward} - if some part of the application lifecycle is painful, it should be done more often, certainly not left to do at the end of the project\footnote{\cite{book-cicd}, p. 26}.
\item \textbf{Idempotency} - the tools used for automation should be idempotent, which means that no matter how many times the tool is invoked, the result should stay the same\footnote{\cite{book-iac}, p. 131}.
\end{itemize}

\paragraph{}
Together with automation, there are two inextricably entwined terms: Infrastructure as Code and DevOps. As these two terms has been already explained in this work, now let us focus on the essential tools needed to introduce the automated application lifecycle. First, a framework for \textbf{Configuration Management} is needed. Examples involve: Puppet, CFEngine\footnote{\cite{book-cicd}, p. 53, \cite{book-iac}, p. 127}, Chef\footnote{\cite{online-chef}}, Ansible\footnote{\cite{online-ansible}}, SaltStack\footnote{\cite{online-salt}}, etc. These tools help to declaratively define what packages should be installed and how should they be configured in a virtual machine or a container or a physical server\footnote{\cite{book-cicd}, p. 53}. They can help prevent \textbf{configuration drift} in a large number of computers\footnote{\cite{book-devops-k8s}, p. 51}. A configuration drift is a difference across systems that were once identical. It can be imposed by a manual amendment and also by automation tools which propagated a change only to some of the instances\footnote{\cite{book-iac}, p. 59, 60}. There are also stack-oriented tools, which follow the same declarative model: Terraform and CloudFormation\footnote{\cite{book-iac}, p. 127}. Another type of needed tools is a building server, examples are: Jenkins, GoCD, Travis. todo: add cite

\textbf{Security} is another essential aspect of production deployment and, as mentioned above, it touches many levels. A node breach is a very serious problem and it can happen by someone logging to the instance or having physical access to it. The latter is easily mitigated by not deploying on bare-metal machines but on a cloud instead\footnote{\cite{book-mastering-k8s}, p. 92, 93}. The former demands some hardening done. Several ideas that can be implemented for a Kubernetes cluster specifically are:
\begin{itemize}
\item ensuring that data is encrypted in transit by using secure api server protocol (HTTPS instead of HTTP)\footnote{\cite{book-mastering-k8s}, p. 93, 113}
\item ensuring proper user and permissions management by configuring authentication, authorization, security accounts and admission control in api server \footnote{\cite{book-mastering-k8s}, p. 93, 98, 104}. When setting up authorization, it is wise to apply \textbf{the principle of least privilege}. This principle recommends that only the needed resources or permissions should be granted\footnote{\cite{book-cndwk}, p. 141}.
\item utilizing Role-Based Access Control (RBAC) to manage access to a cluster\footnote{\cite{book-cndwk}, p. 199}.
\item ensuring security keys management and exchange\footnote{\cite{book-mastering-k8s}, p. 93} by implementing for example automated key rotation
\item ensuring that used Docker images are neither malicious (deliberately causing some harm) nor vulnerable (allowing some attacker to take control) by keeping them up-to-date and maintaining them instead of using the publicly available ones or by using a private Docker registry\footnote{\cite{book-mastering-k8s}, p. 95, 96, 105}
\item using minimal Docker images because the fewer programs there are installed in an image, the fewer potential vulnerabilities there are\footnote{\cite{book-cndwk}, p. 26}.
\item maintaining a log or audit system\footnote{\cite{book-mastering-k8s}, p. 98}
\item utilizing network policies which act in a whitelist fashion and can open certain protocols and ports\footnote{\cite{book-mastering-k8s}, p. 113}
\item using secrets. Kubernetes has a resource called: secret, but the problem is, that Kubernetes stores secrets as plaintext in Etcd. This, in turn, means that steps should be taken in order to limit direct access to Etcd\footnote{\cite{book-mastering-k8s}, p. 113}.
\item prefering managed services, because they will have many security measures already implemented\footnote{\cite{book-cndwk}, p. 41}.
\item avoid running processes as root user in Docker containers\footnote{\cite{book-cndwk}, p. 105, 142}.
\item using available programs for security scanning\footnote{\cite{book-cndwk}, p. 204}.
\end{itemize}

The following illustration presents security features available by api server:
\begin{figure}[H]
    \centering
    \includegraphics[width=16cm]{figures/security-api-server.png}
    \label{fig:security-api-server}
    \caption{An example test and release process diagram}
    \small{Source: \cite{book-mastering-k8s}, p. 101}
\end{figure}

There are many more security measures that Kubernetes administrators and end-users could apply. An interested reader is referred to\cite{book-cndwk}, p. 141-151, 199-205 and \cite{book-mastering-k8s} p. 91-119.

When it comes to backup, rto, rpo


\paragraph{}
Deploying into testing environment and into production environment differ. However, the same process should be followed, despite of which environment is the target.

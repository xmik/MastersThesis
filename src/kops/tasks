#!/bin/bash

set -e

# any environment variable containing non-numeric characters should be unset,
# because it impedes the "kops create cluster" command
unset OS_PASSWORD

export K8S_EXP_REGION="eu-west-1"
# there will be 1 s3 bucket to keep k8s clusters config, for many clusters
# (for many environments)
export K8S_EXP_KOPS_S3_BUCKET="k8s-kops-for-masters-thesis.k8s.local"
export K8S_EXP_VELERO_S3_BUCKET="${K8S_EXP_KOPS_S3_BUCKET}"

if [[ "${K8S_EXP_ENVIRONMENT}" != "testing" ]] && [[ "${K8S_EXP_ENVIRONMENT}" != "production" ]]; then
  echo "Error: K8S_EXP_ENVIRONMENT should be set to testing or production"
  exit 1
fi

export K8S_EXP_CLUSTER_NAME="${K8S_EXP_ENVIRONMENT}.${K8S_EXP_KOPS_S3_BUCKET}"

command="$1"
case "${command}" in
    _gen_config)
        if aws s3api list-buckets | jq '.Buckets[]["Name"]' | grep ${K8S_EXP_KOPS_S3_BUCKET}; then
          echo "S3 Bucket already exists"
        else
          echo "Creating S3 Bucket: ${K8S_EXP_KOPS_S3_BUCKET}"
          set -x
          aws s3api create-bucket --bucket ${K8S_EXP_KOPS_S3_BUCKET} --region ${K8S_EXP_REGION} --create-bucket-configuration LocationConstraint=${K8S_EXP_REGION}
          # enable s3 bucket versioning.
          # strongly recommended in: https://aws.amazon.com/blogs/compute/kubernetes-clusters-aws-kops/
          # https://kops.sigs.k8s.io/getting_started/aws
          aws s3api put-bucket-versioning --bucket ${K8S_EXP_KOPS_S3_BUCKET} --versioning-configuration Status=Enabled
          # enable s3 default encryption (AES-256, SSE-S3)
          # https://kops.sigs.k8s.io/getting_started/aws
          aws s3api put-bucket-encryption --bucket ${K8S_EXP_KOPS_S3_BUCKET} --server-side-encryption-configuration '{"Rules":[{"ApplyServerSideEncryptionByDefault":{"SSEAlgorithm":"AES256"}}]}'
          set +x
        fi
        set -x
        # backup the old config
        if [[ -f cluster.kops-tmpl.yaml ]]; then
          mv cluster.kops-tmpl.yaml cluster-old.kops-tmpl.yaml
        fi
        # generate a cluster configuration
        my_ip=$(curl https://ipinfo.io/ip 2>/dev/null)
        kops create cluster --state "s3://${K8S_EXP_KOPS_S3_BUCKET}" \
        --kubernetes-version=1.16.9 \
        --master-zones="eu-west-1a" --master-count=1 --master-size=t2.micro \
        --zones=eu-west-1a --node-count=1 --node-size=t2.micro \
        --ssh-access=${my_ip}/32 \
        ${K8S_EXP_CLUSTER_NAME}
        # export the configuration to a YAML file
        kops get -o yaml --name ${K8S_EXP_CLUSTER_NAME} --state "s3://${K8S_EXP_KOPS_S3_BUCKET}" > cluster.kops-tmpl.yaml
        echo "Generated config to cluster.kops-tmpl.yaml. You can now edit it to set kubernetesApiAccess, cloudLabels and additionalPolicies"
        # now run the task: _up_remote_config
        ;;
    _up_remote_config)
        # this task updates the running cluster
        kops toolbox template --name ${K8S_EXP_CLUSTER_NAME} --set environment=${K8S_EXP_ENVIRONMENT} --template cluster.kops-tmpl.yaml --format-yaml > cluster-${K8S_EXP_ENVIRONMENT}.yaml
        kops replace -f cluster-${K8S_EXP_ENVIRONMENT}.yaml --name ${K8S_EXP_CLUSTER_NAME} --state "s3://${K8S_EXP_KOPS_S3_BUCKET}" --force
        # for HA mode:
        # kops toolbox template --name ${K8S_EXP_CLUSTER_NAME} --set environment=${K8S_EXP_ENVIRONMENT} --template cluster-ha.kops-tmpl.yaml --format-yaml > cluster-ha-${K8S_EXP_ENVIRONMENT}.yaml
        # kops replace -f cluster-ha-${K8S_EXP_ENVIRONMENT}.yaml --name ${K8S_EXP_CLUSTER_NAME} --state "s3://${K8S_EXP_KOPS_S3_BUCKET}" --force
        ;;
    _create)
        # deploy the cluster; --force is needed if "kops create cluster" was not run before
        kops create secret --name ${K8S_EXP_CLUSTER_NAME} --state "s3://${K8S_EXP_KOPS_S3_BUCKET}" sshpublickey admin -i ~/.ssh/id_rsa.pub
        kops replace -f cluster-${K8S_EXP_ENVIRONMENT}.yaml --name ${K8S_EXP_CLUSTER_NAME} --state "s3://${K8S_EXP_KOPS_S3_BUCKET}" --force
        # kops replace -f cluster-ha-${K8S_EXP_ENVIRONMENT}.yaml --name ${K8S_EXP_CLUSTER_NAME} --state "s3://${K8S_EXP_KOPS_S3_BUCKET}" --force
        # print date, so that end user can expect how much longer to wait
        date
        kops update cluster ${K8S_EXP_CLUSTER_NAME} --state "s3://${K8S_EXP_KOPS_S3_BUCKET}" --yes
        # wait for the deployment to finish
        date
        while [ 1 ]; do
          kops validate cluster ${K8S_EXP_CLUSTER_NAME} --state "s3://${K8S_EXP_KOPS_S3_BUCKET}" && break || sleep 30
        done;
        date
        # does NOT work!
        # enable monitoring with Kubernetes Dashboard
        # kubectl apply -f k8s-dashboard.yaml
        #         dashboard_secret=$(kops get secrets kube --type secret -oplaintext --name ${K8S_EXP_CLUSTER_NAME} --state "s3://${K8S_EXP_KOPS_S3_BUCKET}")
        # echo "Kubernetes Dashboard secret is: ${dashboard_secret}"
      #  aws put-metric-alarm --alarm-name
        ;;
    _enable_logging)
        set -x
        # enable logging to AWS CloudWatch
        # https://awscli.amazonaws.com/v2/documentation/api/latest/reference/logs/create-log-group.html
        aws logs create-log-group --log-group-name k8s-kops-${K8S_EXP_ENVIRONMENT}
        helm install "kube2iam" --namespace="default" --wait --atomic --set rbac.create=true stable/kube2iam
        helm repo add incubator https://kubernetes-charts-incubator.storage.googleapis.com
        helm install "fluentd-cloudwatch" --namespace="default" --wait --atomic --set awsRegion=${K8S_EXP_REGION},rbac.create=true,logGroupName=k8s-kops-${K8S_EXP_ENVIRONMENT} incubator/fluentd-cloudwatch
        ;;
    _list_instances)
        aws ec2 describe-instances --filters "Name=tag-key,Values=deployment" --query "Reservations[*].Instances[*].{PublicIP:PublicIpAddress,Name:Tags[?Key=='Name']|[0].Value,Status:State.Name}"
        ;;
    _enable_as)
        set -x
        sed "s/\${CLUSTER_NAME}/${K8S_EXP_CLUSTER_NAME}/" ../as/cluster-autoscaler-autodiscover.tmpl.yaml > cluster-autoscaler-autodiscover.yaml
        kubectl apply -f cluster-autoscaler-autodiscover.yaml
        ;;
    _install_velero_cli)
        wget -O /tmp/velero.tar.gz https://github.com/vmware-tanzu/velero/releases/download/v1.4.0/velero-v1.4.0-linux-amd64.tar.gz
        cd /tmp
        tar -xvf velero.tar.gz
        chmod +x velero/velero
        sudo mv velero/velero /usr/local/bin/velero
        ;;
    _enable_velero)
        if aws s3api list-buckets | jq '.Buckets[]["Name"]' | grep ${K8S_EXP_VELERO_S3_BUCKET}; then
          echo "Error: S3 Bucket: ${K8S_EXP_VELERO_S3_BUCKET} does not exist"
          exit 1
        fi
        set -x
        kubectl create namespace velero
        helm repo add vmware-tanzu https://vmware-tanzu.github.io/helm-charts
        helm install velero --namespace velero -f ${PWD}/../velero/velero-chart-values.yaml \
          --set configuration.backupStorageLocation.bucket=${K8S_EXP_VELERO_S3_BUCKET} \
          --set configuration.backupStorageLocation.prefix=${K8S_EXP_ENVIRONMENT} \
          --set configuration.backupStorageLocation.config.region=${K8S_EXP_REGION} \
          --set-file credentials.secretContents.cloud=credentials-velero \
          --wait --atomic vmware-tanzu/velero
        ;;
    _test)
        set -x
        time kops validate cluster ${K8S_EXP_CLUSTER_NAME} --state "s3://${K8S_EXP_KOPS_S3_BUCKET}"
        cd ../tests
        time bats tests.bats
        cd ../kops
        ;;
    _delete)
        # print date, so that end user can expect how much longer to wait
        date
        set -x
        kops delete cluster --name ${K8S_EXP_CLUSTER_NAME} --state "s3://${K8S_EXP_KOPS_S3_BUCKET}" --yes
        aws logs delete-log-group --log-group-name k8s-kops-${K8S_EXP_ENVIRONMENT}
        ;;
    *)
        echo "Invalid command: '${command}'"
        exit 1
        ;;
esac

# TODO: delete - delete velero users and s3 buckets; and users keys? aws iam delete-user --user-name Bob
